import PyPDF2 as pdf
import numpy as np
import nltk
import string
import random
import os
import json


# def combine_pdfs(pdf_files):
#     combined_text = ""
#     for file_path in pdf_files:
#         with open(file_path, 'rb') as file:
#             pdf_reader = pdf.PdfReader(file)
#             for page in pdf_reader.pages:
#                 text = page.extract_text()
#                 combined_text += text
#     return combined_text

def adira(sentence):
    # pdf_files = [
    #     "acts.pdf",
    #     "SexualHarassmentofWomenatWorkPlaceAct2013_0.pdf",
    #     "The_Criminal_Law_Amendment_Act_2013_0.pdf",
    #     "TheCommissionofSatiPreventionAct1987-of1988_0.pdf",
    #     "THEDOWRYPROHIBITIONACT1961_0.pdf",
    #     "THEIMMORALTRAFFIC(PREVENTION)ACT1956_2.pdf",
    #     "TheIndecentRepresentationofWomenProhibitionAct1986_2.pdf",
    #     "TheProtectionofWomenfromDomesticViolenceAct2005_0.pdf"
    # ]
    # combined_text = combine_pdfs(pdf_files)
    # output_text_path = "combined_text.txt"
    # with open(output_text_path, "w") as output_file:
    #     output_file.write(combined_text)

    f = open('combined_text.txt')
    raw_doc = f.read()
    # Preprocessing
    # lower case
    raw_doc = raw_doc.lower()
    # tokenisation
    nltk.download('punkt')  # tokeniser
    nltk.download('wordnet')  # dictionary
    nltk.download('omw-1.4')
    sentence_tokens = nltk.sent_tokenize(raw_doc)
    word_tokens = nltk.word_tokenize(raw_doc)
    # lemmatization
    lemmer = nltk.stem.WordNetLemmatizer()
    def LemTokens(tokens):
        return [lemmer.lemmatize(token) for token in tokens]
    remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)
    def LemNormalize(text):
        return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))

    # Define Greeting functions
    greet_inputs = ('hello', 'hi', 'namaste', 'satsriyakal')
    greet_responses = ('Hi', 'Hey', 'Hey There!', 'Namaste')
    def greet(sentence):
        for word in sentence.split():
            if word in sentence.split():
                if word.lower() in greet_inputs:
                    return random.choice(greet_responses)

    # Response generated by the bot
    from sklearn.feature_extraction.text import TfidfVectorizer  # convert the words into nums priority given to rarely occuring words
    from sklearn.metrics.pairwise import cosine_similarity # to measure the similarity of two sentences

    # generating response
    def response(user_response):
        robol_response = ''
        TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')  # tokenisation of the user input
        tfidf = TfidfVec.fit_transform(sentence_tokens)
        vals = cosine_similarity(tfidf[-1], tfidf)
        idx = vals.argsort()[0][-2]
        flat = vals.flatten()
        flat.sort()
        req_tfidf = flat[-2]
        if(req_tfidf == 0):
            robol_response = robol_response + "I am sorry. Unable to understand you!"
            return robol_response
        else:
            robol_response = robol_response + sentence_tokens[idx]
            return robol_response

    # Start conversation
    flag = True
    output = ""
    while(flag == True):
        user_response = sentence
        if(user_response != 'bye'):
            if(user_response == 'thank you' or user_response == 'thankyou' or user_response == 'thanks'):
                flag = False
                output += "You're welcome"
            elif "adira" in user_response:
                flag = False
                output += "ADIRA is a female women safety ai chatbot designed by Anay , Debradita , Aadyaa, Pratap and Himanshi, under supervision of Dr Shilpa Suman , to aid women safety!!!."
            else:
                flag = False
                if(greet(user_response) != None):
                    output += greet(user_response)
                else:
                    sentence_tokens.append(user_response)
                    word_tokens = word_tokens + nltk.word_tokenize(user_response)
                    final_words = list(set(word_tokens))
                    response_text = response(user_response)
                    sentence_tokens.remove(user_response)
                    output += response_text
        else:
            flag = False
            output += "Good Bye"




    # reponse-> openapi-> query, response-> reframe karke de ->output->response return 
    # unable -> openapi-> query-> reponse -short n, simple -> output-> response return 

    return output

# Call the adira function and store its return value in a variable
output = adira("hi")

# Return the output
print(output)














